!
!  mpi_routines.f90
!
!  Free-Format Fortran Source File 
!  Generated by PGI Visual Fortran(R)
!  6/12/2017 2:32:40 PM
!
!  This module contains subroutines for MPI parallelization of the time and space discretization of the 
!  1D3V RK-AB/DG/NDG (t,x,v) colser. MPI parallelization of subroutines responsible for velocity descretization is in 
!  DGV-mpiroutines. Note that the routines below depend on variables set up using subroutines of DGV_mpiroutines
!  
!
!
!!!!!!!!!!!!!!!!!!!
module mpi_routines

use nrtype ! contains kind parameters (DP), (DP), (I4B) etc. 

implicit none

contains 

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! RestartSolutionMTS1D3D_MPI
!
! This is a copy of the restart read from the RKDG driver for 1D3V code adopted for MPI 
! parallelization int eh velocity variable. 
!
! Essentially, each processor will read only those portions of the file that 
! pertain to it.  
!
!!!!!!!!!!!!!!!!!!!
!
! This procedure reads a solution from the disk. 
!
! This subroutine allocates the storage for the solution  
!
! the directory path is set through parameter current_solution_dir
! the name is composed from the curr_sol_base_name and a values of different parameters 
!
! This procedure is dependent on the main program! Do not call before all parameters are 
! set especially f -- this has to have been allocated correctly, its dimensions will be used to 
! set dimensions of other arrays. If the dimensions are wrong, the arrays will not read correctly  
! 
!
! most of the variables are taken directly from 
! commvar.
!
!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

subroutine RestartSolutionMTS1D3D_MPI(restart_time_txt,irank)

use common_variables_mod, only: xmesh,k,N,rk,ftil,ftil1,ftil2,ftil3,ftil4,&
                                frhs1,frhs2,frhs3,frhs4,frhs5,dt   
use read_write_tools_mod
!!!
use DGV_commvar, only: nodes_u,num_lin_proc,lin_proc_nodes_wld,MPI_LINEAR_COLL_WORLD

!
!!!
include 'mpif.h' ! THIS IS TO ENABLE THE MPI ROUTINES
!!!                    
!
intrinsic Trim, Adjustl
!
character (len=20), intent (in) :: restart_time_txt ! the time at which the solution need to be restored
integer, intent (in) :: irank !  MPI variable -- the number of the processor that runs this code.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

character (len=132) :: file_name ! the variable to store the file name 
character (len=20) :: parchar    ! string to keep char value of the parameter 
integer (I4B) :: dump1,dump2     ! string to read some garbage bits from the file
integer (I4B) :: M,M1,k1,N1 ! scrap variables to keep important comstants
integer :: code_line !! local variable to keep the status of reading from file
integer :: loc_alloc_stat ! to keep allocation status
!!!!!!!!!!!!!!!!!!!!!!!!
! Variables specific for MPI algorithms
!!!!!!!!!!!!!!!!!!!!!!!
integer :: fh ! Variable to keep pointer to the open file
integer (I4B), dimension(:), allocatable :: lint_buff
real (DP), dimension(:), allocatable :: dp_buff
integer :: size_lint, size_dp ! cvariables to keep sizes of Fortran types
integer (I4B) :: buff_size !
integer, dimension (MPI_STATUS_SIZE) :: istatus
integer :: ierr ! status variable 
integer (kind=MPI_OFFSET_KIND) :: offset ! variable to keep the offset from the beginning of the file to read                                        
integer (I4B), dimension(0:2) :: gszs,lszs,lstrts ! gszs -- array of sizes for the global array. Global array is divided between MPI processors
 ! lszs are the sizes of the local arrays, lstarts is the array of starting points of local subarrays in the gloab array 
 ! for each dimenstion 
integer :: filetype      ! this is a filetype that is used for MPI IO
! These are the arrays of     

                                             
if (irank < num_lin_proc) then 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !
 ! First, we need to allocate the storages for the main variables
 M=size(lin_proc_nodes_wld,1) ! number of velocity nodes, also the second dimension of ftil 
 ! Note that in the MPI version of the code only portion o fthe velocity nodes is used on a processor
 SELECT CASE(rk)
	CASE(1)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:k,M,N),ftil(0:k,M,N),ftil1(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs1,ftil,ftil1)"
		stop
	  END IF
	CASE(2)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:k,M,N),frhs2(0:k,M,N),ftil(0:k,M,N),ftil1(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs1,frhs2,ftil,ftil1)"
		stop
	  END IF
	CASE(3)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:k,M,N),ftil(0:k,M,N),ftil1(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs1,ftil,ftil1)"
		stop
	  END IF
	ALLOCATE(frhs2(0:k,M,N),frhs3(0:k,M,N),ftil2(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs2,ffrhs3,ftil2)"
		stop
	  END IF
	CASE(4) 
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:k,M,N),ftil(0:k,M,N),ftil1(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs1,ftil1,ftil)"
		stop
	  END IF
	ALLOCATE(frhs2(0:k,M,N),ftil2(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs2,ftil2)"
		stop
	  END IF
	  ALLOCATE(frhs3(0:k,M,N),frhs4(0:k,M,N),ftil3(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs3,ftil3)"
		stop
	  END IF
    CASE(5)
      !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:k,M,N),ftil1(0:k,M,N),ftil(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs1,ftil1,ftil)"
		stop
	  END IF
	  ALLOCATE(frhs2(0:k,M,N),ftil2(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs2,ftil2)"
		stop
	  END IF
	ALLOCATE(frhs3(0:k,M,N),ftil3(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs3,ftil3)"
		stop
	  END IF
	  ALLOCATE(frhs4(0:k,M,N),frhs5(0:k,M,N),ftil4(0:k,M,N), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"RestartSolutionMTS1D3D_MPI: Allocation error for variable (frhs4,frhs5,ftil4)"
		stop
	  END IF
	CASE default
			PRINT *, "RestartSolutionMTS1D3D_MPI: The value of (rk) must be from 1 to 5. Stop"
			stop
 END SELECT 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ! The storage for the main variables has been created. Now, we need 
 ! to read the solution file into memory. Since the solution file may be big, the processors will take turns
 ! at reading it. They will create temporary full size arrays, read the entire file and only retain portions that are relevant to them 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 
 !!! WE will use MPI-2 Parallel file read features in this subroutine
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 allocate (lint_buff(0:10),dp_buff(0:N), stat=loc_alloc_stat)  !
 if (loc_alloc_stat >0) then 
  print *, "RestartSolutionMTS1D3D_MPI: Allocation error for variable (buffer). Process", irank, ". Stop"
  stop
 end if 
 lint_buff=0
 dp_buff=0
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !
 ! Second, we prepare the file name to read the solution
 call MakeBaseNameHOV1D(file_name)
 file_name = trim(Adjustl(file_name))//trim(Adjustl(restart_time_txt))// "time"
 file_name = trim(file_name)//"_1D3Dsol.dat" ! this file will keep the array 
 !
 ! now we open the file for record and save some stuff in it: 
 call MPI_FILE_OPEN (MPI_LINEAR_COLL_WORLD, file_name, MPI_MODE_RDONLY, MPI_INFO_NULL, fh, ierr)
 !call MPI_Sizeof(dump1,size_lint,ierr) ! figure the size in bytes of the long integer
 size_lint=4 ! It seems that MPI_SizeOf is not supported by my old MPI
 !call MPI_Sizeof(dp_buff,size_dp,ierr) ! figure the size in bytes of the double precision
 size_dp=8 
 offset = size_lint ! Start reading from the beginning (try to check if the 32 bits need to be skipped.
 call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
 !Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
 ! uncomment only one line
 !call MPI_FILE_READ(fh,lint_buff,2,MPI_LONG,istatus,ierr) ! read the first two long integers
 call MPI_FILE_READ(fh,lint_buff,2,MPI_INTEGER4,istatus,ierr) ! read the first two long integers  Microsoft MPI does not know MPI_LONG
 k1=lint_buff(0)
 N1=lint_buff(1)
 offset=offset+2*size_lint
 call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr) 
 call MPI_FILE_READ(fh,dp_buff,N+1,MPI_DOUBLE_PRECISION,istatus,ierr)   ! then read the array of N+1 double reals
 xmesh(0:N)=dp_buff(0:N)
 offset=offset+(N1+1)*size_dp
 call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
 !Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
 ! uncomment only one line
 !call MPI_FILE_READ(fh,lint_buff,2,MPI_LONG,istatus,ierr)   ! then read two more long integer       
 call MPI_FILE_READ(fh,lint_buff,2,MPI_INTEGER4,istatus,ierr)   ! then read two more long integer
 M1=lint_buff(0)
 rk=lint_buff(1)
 offset=offset+2*size_lint
 call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
 call MPI_FILE_READ(fh,dp_buff,1,MPI_DOUBLE_PRECISION,istatus,ierr)   ! then read 1 double real
 dt=dp_buff(0)
 !!! serial read command:  read (15,  iostat = code_line) k1,N1,xmesh,M1,rk,dt
 ! A quick check if the saved solution is stored correctly.
 if ((k1 /= k) .or. (N1 /= N) .or. (M1 /= size(nodes_u,1))) then 
  if (irank==0) then 
   print *, "RestartSolutionMTS1D3D_MPI: Constants k,M and N in the saved solutions are incompatible.",&
    "k1,k=",k1,k,"N1,N=",N1,N,"M1,M,#allnds",M1,M,size(nodes_u,1)
  end if 
  call MPI_FILE_CLOSE(fh,ierr)
  stop
 end if 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ! now goes the solution, which we need to read into local arrays !!!
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ! First, we create the filetype that will be used by the MPI IO to read a portion of the array
 gszs(0) = k+1 !size of the global array in dim 1
 gszs(1) = size(nodes_u,1) !size of the global array in dim 2
 gszs(2) = N   !size of the global array in dim 3
 lszs(0) = k+1 ! size of the local array in dim 1
 lszs(1) = M   ! size of the local array in dim 2   
 lszs(2) = N   ! size of the local array in dim 3
 lstrts(0) = 0 ! starting index of the local array in dim 1
 lstrts(1) = lin_proc_nodes_wld(1)-1 ! We assume that nodes numbers are ordered in lin_proc_nodes_wld
 lstrts(2) =0  ! starting index of the local array in dim 3
 call MPI_TYPE_CREATE_SUBARRAY(3,gszs,lszs,lstrts,MPI_ORDER_FORTRAN,MPI_DOUBLE_PRECISION,filetype,ierr) 
 call MPI_TYPE_COMMIT(filetype,ierr)
 !!! Next we set the view and read the piece of array
 ! We will do it once for each array to read 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 offset = 4 + 2*size_lint + (N+1)*size_dp + 2*size_lint + size_dp + 8 ! estimate displacement to the very first record of the very first array to read
 buff_size=M*(k+1)*N  ! number of records to read
 SELECT CASE(rk) 
 CASE(1)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(2)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(3)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(4)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(5)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,ftil4,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_READ_ALL(fh,frhs4,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE default 
  PRINT *, "Unsupported degree of Multiple Time Stepping method passed"
  call MPI_FILE_CLOSE(fh,ierr)
  stop
 END SELECT    
 !!! Done reading file, can close the file now
 call MPI_FILE_CLOSE(fh,ierr)
 deallocate(lint_buff,dp_buff)
else  !a small sanity check -- processors with irank>=num_lin_procs should not call this subroutine
 print *,"RestartSolutionMTS1D3D_MPI: Error -- this processor should not envoke this routine.Stop"
 stop
end if

end subroutine RestartSolutionMTS1D3D_MPI

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! WriteSolutionMTS1D3D_MPI
!
! This subroutine is saving the current solution on the hard drive. The MPI parallel IO is 
! used to write the file.
!
! the directory path is set through parameter current_solution_dir
! the name is composed from the curr_sol_base_name and a values of different parameters 
!
! This procedure is dependent on the main program! Do not call before all parameters are 
! set especially ftil -- this has to have been allocated correctly, its dimensions will be used to 
! set dimensions of other arrays. If the dimensions are wrong, the arrays will not read correctly  
! 
!
! most of the variables are taken directly from 
! commvar.
!
!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

subroutine WriteSolutionMTS1D3D_MPI(restart_time_txt,irank)

use common_variables_mod, only: xmesh,k,N,rk,ftil,ftil1,ftil2,ftil3,ftil4,&
                                frhs1,frhs2,frhs3,frhs4,frhs5,dt   
use read_write_tools_mod                                
!!!
use DGV_commvar, only: nodes_u,num_lin_proc,lin_proc_nodes_wld,MPI_LINEAR_COLL_WORLD
!
!!!
include 'mpif.h' ! THIS IS TO ENABLE THE MPI ROUTINES
!!!                    
!
intrinsic Trim, Adjustl
!
character (len=20), intent (in) :: restart_time_txt ! the time at which the solution need to be restored
integer, intent (in) :: irank !  MPI variable -- the number of the processor that runs this code.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

character (len=132) :: file_name ! the variable to store the file name 
character (len=20) :: parchar    ! string to keep char value of the parameter 
integer (I4B) :: dump1,dump2     ! string to read some garbage bits from the file
integer (I4B) :: M,M1,k1,N1 ! scrap variables to keep important comstants
integer :: code_line !! local variable to keep the status of reading from file
integer :: loc_alloc_stat ! to keep allocation status
!!!!!!!!!!!!!!!!!!!!!!!!
! Variables specific for MPI algorithms
!!!!!!!!!!!!!!!!!!!!!!!
integer :: fh ! ZVariable to keep pointer to the open file
integer (I4B), dimension(:), allocatable :: lint_buff
real (DP), dimension(:), allocatable :: dp_buff
integer :: size_lint, size_dp ! cvariables to keep sizes of Fortran types
integer (I4B) :: buff_size !
integer, dimension (MPI_STATUS_SIZE) :: istatus
integer :: ierr ! status variable 
integer (kind=MPI_OFFSET_KIND) :: offset ! variable to keep the offset from the beginning of the file to read                                        
integer (I4B), dimension(0:2) :: gszs,lszs,lstrts ! gszs -- array of sizes for the global array. Global array is divided between MPI processors
 ! lszs are the sizes of the local arrays, lstarts is the array of starting points of local subarrays in the gloab array 
 ! for each dimenstion 
integer :: filetype      ! this is a filetype that is used for MPI IO
! These are the arrays of     
                                            
if (irank < num_lin_proc) then 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !
 ! First, we need to allocate the storages for the main variables
 M=size(lin_proc_nodes_wld,1) ! number of velocity nodes, also the second dimension of ftil 
 ! In the MPI version opf the program only a portion of velocity nodes is used on each processor
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !!! WE will use MPI-2 Parallel file read features in this subroutine
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 allocate (lint_buff(0:4),dp_buff(0:N),stat=loc_alloc_stat)  !
 if (loc_alloc_stat >0) then 
  print *, "RestartSolutionMTS1D3D_MPI: Allocation error for variable (buffer). Process", irank, ". Stop"
  stop
 end if 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 !
 ! Second, we prepare the file name to read the solution
 call MakeBaseNameHOV1D(file_name)
 file_name = trim(Adjustl(file_name))//trim(Adjustl(restart_time_txt))// "time"
 file_name = trim(file_name)//"_1D3Dsol.dat" ! this file will keep the array 
 !
 ! now we open the file for record and save some stuff in it: 
 call MPI_FILE_OPEN (MPI_LINEAR_COLL_WORLD, file_name, MPI_MODE_CREATE+MPI_MODE_WRONLY, MPI_INFO_NULL, fh, ierr)
 !call MPI_Sizeof(dump1,size_lint,ierr) ! figure the size in bytes of the long integer
 size_lint = 4 ! It seems that MPI_SizeOf is not supported by my old MPI
 !call MPI_Sizeof(dp_buff,size_dp,ierr) ! figure the size in bytes of the double precision
 size_dp = 8 
 if (irank==0) then  ! the header is written only on the irank=0 processor
  offset = 0 ! Start reading from the beginning (try to check if the 32 bits need to be skipped.
  lint_buff(0)=4*size_lint+size_dp+(N+1)*size_dp ! Fortran write length of the record in a long integer = # bytes to read. The record ends with its length written, again. -- to mimic Fortran write. 
  lint_buff(1)=k
  lint_buff(2)=N
  call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  ! Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
  ! uncomment only one line
  ! call MPI_FILE_WRITE(fh,lint_buff,3,MPI_LONG,istatus,ierr) ! read the first two long integers
  call MPI_FILE_WRITE(fh,lint_buff,3,MPI_INTEGER4,istatus,ierr) ! read the first two long integers Microsoft MPI does not know MPI_LONG
  dp_buff(0:N)=xmesh(0:N)
  offset=offset+3*size_lint
  call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  call MPI_FILE_WRITE(fh,dp_buff,N+1,MPI_DOUBLE_PRECISION,istatus,ierr)   ! then read the array of N+1 double reals
  lint_buff(0)=size(nodes_u,1)
  lint_buff(1)=rk
  offset=offset+(N+1)*size_dp
  call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  ! Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
  ! uncomment only one line
  ! call MPI_FILE_WRITE(fh,lint_buff,2,MPI_LONG,istatus,ierr)   ! then read two more long integer
  call MPI_FILE_WRITE(fh,lint_buff,2,MPI_INTEGER4,istatus,ierr)   ! then read two more long integer Microsoft MPI does not know MPI_LONG
  dp_buff(0)=dt
  offset=offset+2*size_lint
  call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  call MPI_FILE_WRITE(fh,dp_buff,1,MPI_DOUBLE_PRECISION,istatus,ierr)   ! then read 1 double real
  lint_buff(0)=4*size_lint+size_dp+(N+1)*size_dp ! Fortran write length of the record in bytes at the beginning and at the end of the record. the length is writted as a long integer -- to mimic Fortran write. 
  lint_buff(1)=(1+(rk-1)*2)*(k+1)*N*size(nodes_u,1)*size_dp
  offset=offset+size_dp
  call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  ! Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
  ! uncomment only one line
  ! call MPI_FILE_WRITE(fh,lint_buff,2,MPI_LONG,istatus,ierr) ! write the first two long integers
  call MPI_FILE_WRITE(fh,lint_buff,2,MPI_INTEGER4,istatus,ierr) ! write the first two long integers Microsoft MPI does not know MPI_LONG
  !!! serial write command:  write (15) k,N,xmesh,size(ftil,2),rk,dt ! the last number is the total number of velocity nodes
 end if 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ! now goes the solution, which we need to save from local arrays !!!
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 ! First, we create the filetype that will be used by the MPI IO to read a portion of the array
 gszs(0) = k+1 !size of the global array in dim 1
 gszs(1) = size(nodes_u,1) !size of the global array in dim 2
 gszs(2) = N   !size of the global array in dim 3
 lszs(0) = k+1 ! size of the local array in dim 1
 lszs(1) = M   ! size of the local array in dim 2   
 lszs(2) = N   ! size of the local array in dim 3
 lstrts(0) = 0 ! starting index of the local array in dim 1
 lstrts(1) = lin_proc_nodes_wld(1)-1 ! We assume that nodes numbers are ordered in lin_proc_nodes_wld
 lstrts(2) =0  ! starting index of the local array in dim 3
 call MPI_TYPE_CREATE_SUBARRAY(3,gszs,lszs,lstrts,MPI_ORDER_FORTRAN,MPI_DOUBLE_PRECISION,filetype,ierr) 
 call MPI_TYPE_COMMIT(filetype,ierr)
 !!! Next we set the view and read the piece of array
 ! We will do it once for each array to read 
 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 offset = 4 + 2*size_lint + (N+1)*size_dp + 2*size_lint + size_dp+ 8 ! estimate displacement to the very first record of the very first array to read
 buff_size=M*(k+1)*N  ! number of records to read
 SELECT CASE(rk) 
 CASE(1)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(2)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(3)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(4)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE(5)
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,ftil4,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs1,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs2,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs3,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
  offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  call MPI_FILE_SET_VIEW(fh,offset,MPI_DOUBLE_PRECISION,filetype,'native',MPI_INFO_NULL,ierr)
  call MPI_FILE_WRITE_ALL(fh,frhs4,buff_size,MPI_DOUBLE_PRECISION,istatus,ierr) 
 CASE default 
  PRINT *, "WriteSolutionMTS1D3D_MPI: Unsupported degree of Multiple Time Stepping method passed"
  call MPI_FILE_CLOSE(fh,ierr)
  stop
 END SELECT    
 !!! Done reading file, can close the file now
 offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
 call MPI_FILE_SET_VIEW(fh,offset,MPI_BYTE,MPI_BYTE,'native',MPI_INFO_NULL,ierr)
 !!! Resetting the File View to original shifted to the last reecord
 
 if (irank==0) then  ! the header is written only on the irank=0 processor
  !offset = offset + (k+1)*size(nodes_u,1)*N*size_dp ! increase the offset to accommodate for the array that was just read
  !call MPI_FILE_SEEK(fh,offset,MPI_SEEK_SET,ierr)
  lint_buff=0
  lint_buff(0)=(1+(rk-1)*2)*(k+1)*N*size(nodes_u,1)*size_dp
  ! Attention: the next two lines do the same and only one is needed. Microsoft MPI does not know MPI_LONG
  ! uncomment only one line
  ! call MPI_FILE_WRITE(fh,lint_buff,1,MPI_LONG,istatus,ierr) ! read the first two long integersMicrosoft MPI does not know MPI_LONG
  call MPI_FILE_WRITE(fh,lint_buff,1,MPI_INTEGER4,istatus,ierr) ! read the first two long integersMicrosoft MPI does not know MPI_LONG
 end if  
 call MPI_FILE_CLOSE(fh,ierr)
 call MPI_TYPE_FREE(filetype,ierr)
 deallocate(lint_buff,dp_buff)
else  !a small sanity check -- processors with irank>=num_lin_procs should not call this subroutine
 print *,"WriteSolutionMTS1D3D_MPI: Error -- this processor should not envoke this routine.Stop"
 stop
end if

end subroutine WriteSolutionMTS1D3D_MPI

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! Set1DHOVinitftil_MPI
!
! This is a copy of Set1DHOVinitftil adjusted for MPI parallelization. 
!
! This subroutine sets up the initial data for ftil1 and ftil2 arrays. Do not detouch from the main program 
! 
!
! This subroutine is highly dependent on the main program. It is created mainly 
! to organize the main program. 
! 
! The subroutine can not be called before the parameters umesh, xmesh, s,k, max_degree, selected_exact_sol, curr_time are selected!! 
! 
! Most of the varaibles are looked up in the common_varaibles_module
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
subroutine Set1DHOVinitftil_MPI
use common_variables_mod, only: k,xmesh,N,ftil,curr_time

use DGV_commvar, only: nodes_u,nodes_v,nodes_w,lin_proc_nodes_wld,&
					   nodes_u_loc,nodes_v_loc,nodes_w_loc,&
					   MPI_LINEAR_COLL_WORLD !  variables to keep coordinated to nodes assigend to this processor. 
use DGV_sf02, only: f_1D3D


use spectral_tools_mod

!!!
intrinsic min
!!!
integer :: loc_alloc_stat ! to keep allocation status
!real (DP), dimension (0:s) :: ff2 ! dump variable to compute matrix product
!!!
!  first, we prepare the space for the solution: ftil(p,j,m):
                                                      ! -- p is the index in the basis functions in x 
                                                      ! -- m is the index in the basis functions in u/nodes in u
                                                      ! -- j is the cell in x
allocate (ftil(0:k,size(lin_proc_nodes_wld,1),N), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "Set1DHOVinitftil_MPI: Allocation error for variable (ftil)"
     end if 
     !
! now we must set up ftil 
! the first step is to calculate the spectal coefficients of the intial data
ftil = DecompLeg1D3D(xmesh,nodes_u_loc,nodes_v_loc,nodes_w_loc,k,f_1D3D,curr_time)	! the main distribution, 
!
!!!!

end subroutine Set1DHOVinitftil_MPI

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! subroutine  PrepareMTS1D3D_MPI
!
! This is a copy of PrepareMTS1D3D adjusted for MPI parallelization. 
!
! Subroutine prepares the intermediate time steps to be used in the multi-step methods.
! f stores the actual solution.
! f1, f2, f3, f4, frhs1, frhs2, frhs3, frhs4 store the value of the 
!	spatial operator and the right hand side on those intermediate time steps 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
SUBROUTINE PrepareMTS1D3D_MPI(irank)

use common_variables_mod, only: ftil,ftil1,ftil2,ftil3,ftil4,frhs1,frhs2,frhs3,frhs4, &
                                frhs5,curr_time,dt,rk
!!!!!!! DEBUG 
use DGV_commvar, only: run_mode_1D
!!!!!!!! END DEBUG

integer, intent (in) :: irank ! rank of the this processor in the global communicator

!!!!!! DECLARE LOCAL VARIABLES !!!!!
INTEGER :: i,s1,s2,s3													! Local counters
INTEGER :: loc_alloc_stat ! to keep the allocation status 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! auxiliary numbers -- sizes of the arrays
s1=size(ftil,1)-1
s2=size(ftil,2)
s3=size(ftil,3)

! First, we need to allocate the storages
SELECT CASE(rk)
	CASE(1)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
	  ALLOCATE(frhs1(0:s1,1:s2,1:s3),ftil1(0:s1,1:s2,1:s3),	stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs1), (ftil1)"
		stop
	  END IF
	CASE(2)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:s1,1:s2,1:s3),frhs2(0:s1,1:s2,1:s3),ftil1(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs1,frsh2,ftil1)"
		stop
	  END IF
	CASE(3)
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:s1,1:s2,1:s3),ftil1(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs1,ftil1)"
		stop
	  END IF
	  ALLOCATE(frhs2(0:s1,1:s2,1:s3),frhs3(0:s1,1:s2,1:s3),ftil2(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs2,ftil2,frhs3)"
		stop
	  END IF
	CASE(4) 
	  !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:s1,1:s2,1:s3),ftil1(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs1,ftil1)"
		stop
	  END IF
	  ALLOCATE(frhs2(0:s1,1:s2,1:s3),ftil2(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs2,ftil2)"
		stop
	  END IF
	  ALLOCATE(frhs3(0:s1,1:s2,1:s3),frhs4(0:s1,1:s2,1:s3),ftil3(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs3,ftil3)"
		stop
	  END IF
    CASE(5)
      !!! ALLOCATE THE ARRAYS NEEDED FOR THE MTS STEPS
   	  ALLOCATE(frhs1(0:s1,1:s2,1:s3),ftil1(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs1,ftil1)"
		stop
	  END IF
	  ALLOCATE(frhs2(0:s1,1:s2,1:s3),ftil2(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs2,ftil2)"
		stop
	  END IF
	ALLOCATE(frhs3(0:s1,1:s2,1:s3),ftil3(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs3,ftil3)"
		stop
	  END IF
	  ALLOCATE(frhs4(0:s1,1:s2,1:s3),frhs5(0:s1,1:s2,1:s3),ftil4(0:s1,1:s2,1:s3), stat = loc_alloc_stat)
	  IF(loc_alloc_stat > 0) THEN
		PRINT *,"PrepareMTS1D3D: Allocation error for variable (frhs4,ftil4)"
		stop
	  END IF
	CASE default
			PRINT *, "PrepareMTS1D3D: The value of (rkmts) must be from 1 to 5. No such RK or MTS methods implemented"
			stop
END SELECT 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! Now we assume that f contains the intial data. We need to set up the arrays
! If rkmts=1 - no action is needed all data is in place 
! If the rkmts>2 then we need to actually prepare some arrays. 
do i=2,rk
 !! FIRST WE NEED TO PUSH THE SOLUTION AND THE RIGHT SIDE DOWN THE STORAGE TO OPEN SOME SPACE FOR THE NEW DATA 
 SELECT CASE(rk)
 CASE(2)
  ftil1=ftil 
 CASE(3)
  ftil2=ftil1; ftil1=ftil; frhs2=frhs1
 CASE(4) 
  ftil3=ftil2; ftil2=ftil1; ftil1=ftil; frhs3=frhs2; frhs2=frhs1
 CASE(5) 
  ftil4=ftil3; ftil3=ftil2; ftil2=ftil1; ftil1=ftil; frhs4=frhs3; frhs3=frhs2; frhs2=frhs1
 CASE default
  PRINT *, "PrepareMTS_SH_DGV: The value of (rkmts) must be from 1 to 5. No such RK or MTS methods implemented"
  stop
 END SELECT 
! now, the f contains the solution at the current time. We call the time integrator and then f will contain the next time step and 
! frhs1 will contain the solution at the curent time.
 call TimeIntegratorORK_HOV1D3D_MPI
 curr_time = curr_time+dt ! advance the time (the ORK does not advance time)
 ! DEBUG
 !if (irank==0) then 
 ! print *, "time=", curr_time, "run_mode_1D=", run_mode_1D
 !end if 
 ! END DEBUG 
end do 
END SUBROUTINE PrepareMTS1D3D_MPI

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! TimeIntegratorORK_HOV1D3D_MPI
! 
!  This is a copy of the subroutine TimeIntegratorORK_HOV1D3D adjusted for MPI implementation
!
! This subroutine implements Optimal Runge Kutta integration. 
! 
! It is dependent on the main program: uses variables from the 
! common_variables_mod 
!
! the called subroutines look up varaibles in common_variables_mod 
! 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 

subroutine TimeIntegratorORK_HOV1D3D_MPI

use common_variables_mod, only: curr_time,dt,rk,ftil,frhs1
use spatial_operator_mod
!
real (DP), dimension (:,:,:), allocatable :: ft1_1, ft1_2, ft1_3, ft1_4 ! auxiliary variables to keep intermediate time steps
integer :: loc_alloc_stat ! to keep allocation status
!
!!!!!!!!!!!!!!!!

select case (rk)
 case (1) 
  allocate (ft1_1(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_1)"
     end if 
     !
  !Runger Kutta first order (Euler method)
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_1,ftil,curr_time,dt)     
   frhs1 = ft1_1/dt
   ftil = ftil + ft1_1
  ! end Runge Kutta 1st order (Euler method).   
  deallocate(ft1_1)
 case (2)
  allocate (ft1_1(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_1)"
     end if 
     !
  allocate (ft1_2(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_2)"
     end if 
     !
   !Runge Kutta second order (optimal 2nd order method)
  ! t_1:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_1,ftil,curr_time,dt)
   frhs1 = ft1_1/dt
  ! t_2: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil+ft1_1*2/3_DP,&
                      curr_time+2*dt/3_DP,dt)
  !                    
   ftil = ftil + ft1_1/4_DP + ft1_2*3/4_DP
  ! end optimal Runge Kutta second order 
  deallocate (ft1_1, ft1_2)
 case (3)
  allocate (ft1_1(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_1)"
     end if 
     !
  allocate (ft1_2(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_2)"
     end if 
     !
  allocate (ft1_3(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_3)"
     end if 
     !
  ! t_1:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_1,ftil,curr_time,dt)
   frhs1 = ft1_1/dt
  ! t_2: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil+ft1_1/2_DP,&
                      curr_time+dt/2_DP,dt)
  ! t_3:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_3,ftil+ft1_2*3/4_DP,&
                      curr_time+dt*3/4_DP,dt)     
  !
   ftil = ftil + ft1_1*2/9.0_DP + ft1_2*3/9.0_DP + ft1_3*4/9.0_DP
   ! end optimal Runge-Kutta third order
  deallocate(ft1_1, ft1_2, ft1_3)
 case (4)
  allocate (ft1_1(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_1)"
     end if 
     !
  allocate (ft1_2(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_2)"
     end if 
     !
  allocate (ft1_3(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_3)"
     end if 
     !
  ! (Optimal ?) Runge Kutta fourth order (Ralston and Rabinowitz ??)
  ! t_1:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_1,ftil,curr_time,dt)
   frhs1 = ft1_1/dt
  ! t_2: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil+ft1_1/2_DP,&
                      curr_time+dt/2_DP,dt)
  ! t_3:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_3,ftil+ft1_1/4_DP+ft1_2/4_DP,&
                      curr_time+dt/2_DP,dt)     
  ! t_4: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil-ft1_2+ft1_3*2.0_DP,&
                      curr_time+dt,dt)     
  !
   ftil = ftil + ft1_1/6.0_DP + ft1_3*2/3.0_DP + ft1_2/6.0_DP  ! :2nd componenent is re-used for 4th component!
  ! end Runge-Kutta fourth order (Ralston and Rabinowitz ??) 
  deallocate (ft1_1,ft1_2,ft1_3)
 case (5)
 allocate (ft1_1(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_1)"
     end if 
     !
 allocate (ft1_2(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_2)"
     end if 
     !
 allocate (ft1_3(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_3)"
     end if 
     !
 allocate (ft1_4(size(ftil,1),size(ftil,2),size(ftil,3)), stat=loc_alloc_stat)
     !
     if (loc_alloc_stat >0) then 
     print *, "TimeIntegratorORK_HOV1D3D: Allocation error for variable (ft1_4)"
     end if 
     !
  ! Runge Kutta fifth order (Ralston and Rabinowitz ?? compatible with the fourth order)
  ! t_1:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_1,ftil,curr_time,dt)
   frhs1 = ft1_1/dt
  ! t_2: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil+ft1_1/2_DP,&
                      curr_time+dt/2_DP,dt)
  ! t_3:
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_3,ftil+ft1_1/4_DP+ft1_2/4_DP,&
                      curr_time+dt/2_DP,dt)     
  ! t_4: 
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_4,ftil-ft1_2+ft1_3*2,&
                      curr_time+dt,dt)     
  ! we now introduce some memory savings and replace ft1_3 with -ft1_2/5_DP+ft1_3*546/625_DP that is going to be ised in step 6
   ft1_3 = -ft1_2/5_DP+ft1_3*546/625.0_DP
   !t_5:
  ! on this step, we re-use ft1_2 to take place of ft1_5
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_2,ftil+ft1_1*7/27.0_DP+ft1_2*10/27.0_DP+ft1_4/27.0_DP,&
                      curr_time+dt*2/3_DP,dt)
  !t_6: 
  ! on this step, we re-use ft1_3 to take place of ft1_6
   call SpatialOperatorHOV1D3D_UpwindFlux_MPI(ft1_3,ftil+ft1_1*28/625.0_DP+ft1_3+ft1_4*54/625.0_DP - ft1_2*378/625.0_DP,&
                      curr_time+dt/5_DP,dt)
  !
   ftil = ftil + ft1_1/24_DP + ft1_4*5/48_DP + ft1_2*27/56_DP + ft1_3*125/336_DP
   deallocate (ft1_1,ft1_2,ft1_3,ft1_4) 
  case default
           print *, "unsupported degree of Runge-Kutta method passed"
 end select 
!
end subroutine TimeIntegratorORK_HOV1D3D_MPI

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! TimeIntegratorMTS1D3D_MPI
!
! This is a version of TimeIntegratorMTS1D3D modigied to run in MPI parallel code
! 
! This subroutine implements Multiple Step Time integration. 
! 
! It is dependent on the main program: uses variables from the 
! common_variables_mod 
!
! the called subroutines look up varaibles in common_variables_mod 
! 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 

SUBROUTINE TimeIntegratorMTS1D3D_MPI

use common_variables_mod, only: ftil,ftil1,ftil2,ftil3,ftil4,frhs1,frhs2,frhs3,frhs4, &
                                frhs5,curr_time,dt,rk

SELECT CASE (rk)
	CASE (1)
	 !!! MTS FIRST ORDER (EULER METHOD)
	 CALL SpatialOperatorHOV1D3D_UpwindFlux_MPI(frhs1,ftil,curr_time,dt)     
     !
     ftil = ftil + frhs1 
       !
	 !!! END MTS 1ST ORDER (EULER METHOD)
	CASE (2)
	 !!! ADAMS-BASHFORTH TWO STEP EXPLICIT METHOD
	 ftil1 = ftil; frhs2 = frhs1 !! first we need to push the solution into the storage arrays
	 CALL SpatialOperatorHOV1D3D_UpwindFlux_MPI(frhs1,ftil,curr_time,dt)
	 ftil = ftil + frhs1*3.0_DP/2.0_DP - dt*frhs2/2.0_DP
	 !!! END ADAMS-BASHFORTH TWO STEP EXPLICIT METHOD
	CASE (3)
	 !!! BEGIN ADAMS-BASHFORTH THREE STEP
	 ftil2 = ftil1; ftil1 = ftil; frhs3 = frhs2; frhs2 = frhs1 !! first we need to push the solution into the storage arrays
	 CALL SpatialOperatorHOV1D3D_UpwindFlux_MPI(frhs1,ftil,curr_time,dt) 
	 ftil = ftil + frhs1*23.0_DP/12.0_DP - dt*frhs2*4/3.0_DP + dt*frhs3*5.0_DP/12.0_DP 
    !!! END ADAMS-BASHFORTH THREE STEP
	CASE (4)
   	 !!! ADAMS-BASHFORTH FOUR STEP
	 ftil3 = ftil2; ftil2 = ftil1; ftil1 = ftil; frhs4 = frhs3; frhs3 = frhs2; frhs2 = frhs1 !! first we need to push the solution into the storage arrays
	 CALL SpatialOperatorHOV1D3D_UpwindFlux_MPI(frhs1,ftil,curr_time,dt) 
	 ftil = ftil + frhs1*55.0_DP/24.0_DP - dt*frhs2*59.0_DP/24.0_DP + dt*frhs3*37.0_DP/24.0_DP - dt*frhs4*3.0_DP/8
	 !!! END ADAMS-BASHFORTH FOUR STEP
	CASE (5)
	 !!! ADAMS-BASHFORTH FIVE STEP METHOD
	 ftil4 = ftil3; ftil3 = ftil2; ftil2 = ftil1; ftil1 = ftil 
	 frhs5 = frhs4; frhs4 = frhs3; frhs3 = frhs2; frhs2 = frhs1 !! first we need to push the solution into the storage arrays
	 CALL SpatialOperatorHOV1D3D_UpwindFlux_MPI(frhs1,ftil,curr_time,dt) 
	 ftil = ftil + frhs1*(1901.0_DP/720.0_DP) - dt*frhs2*(2774.0_DP/720.0_DP) + dt*frhs3*(2616.0_DP/720.0_DP) &
	       - dt*frhs4*(1274.0_DP/720.0_DP) + dt*frhs5*(251.0_DP/720.0_DP)
	 !!! END ADAMS-BASHFORTH FIVE STEP METHOD
    CASE default
	 PRINT *, "TimeIntegratorMTS1D3D_MPI: Unsupported degree of Multiple Time Stepping method passed"
	 stop
	END SELECT
    frhs1 = frhs1/dt
    curr_time = curr_time + dt ! advance the time for one step
END SUBROUTINE TimeIntegratorMTS1D3D_MPI



!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! SpatialOperatorHOV1D3D_UpwindFlux_MPI
!
! This is a copy of the subroutine SpatialOperatorHOV1D3D_UpwindFlux adjusted for the MPI implementation
! 
! This function  implement the spatial differential operator of the DG 1D HOV model 
! with the upwind flux. 
! 
! Do not detouch form the main program
! 
! Do not call before k,s,M,N,Ainvml,u_gauss_nodes,u_wleg_pol have been initialized
! 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

subroutine SpatialOperatorHOV1D3D_UpwindFlux_MPI(f1,ftil1,curr_time,tau) 

use common_variables_mod, only: selected_leftbcond,selected_rightbcond,periodic_bc,exact_bc,diffusive_reflection_bc,& 
                                xmesh,k,N,selected_exact_sol
                      
!!!! DGVlib                                
use DGV_sf02
use DGV_commvar, only: nodes_u_loc,nodes_v_loc,nodes_w_loc ! a portion of arrays nodes_u, nodes_v, nodes_w that is used by this processor. Recall that velocity nodes and
																	! therefore components of the transport part are divided between processors.
																	
!!!!!!!!!!!!
																	
use spatial_operator_mod
use boundary_cond_mod
use right_side_mod

! main variables 

real (DP), dimension (0:,:,:), intent (out) :: f1 ! values of the discrete transport operator 
                                 ! -- p is the index in the basis functions in x 
                                 ! -- m is the index in the basis functions in u
                                 ! -- j is the cell in x
real (DP), dimension (0:,:,:), intent (in) :: ftil1 ! main variable main variable -- coefficients of spectral decomposition in 
                                                      !  ftil(p,m,j):
                                                      ! -- p (0:k) is the index in the basis functions in x 
                                                      ! -- m (1:Mu*su*Mv*sv*Mw*sw) is the index in the basis functions in u/velocity nodes - comes from DGVlib
                                                      ! -- j is the cell in x
                                                      
real (DP), intent (in) :: curr_time ! the coordinate time
real (DP), intent (in) :: tau ! the time step = dt                                                                  
! auxiliary variables 
real (DP), dimension (:), allocatable  :: TilFmi1Left   ! values of \tilde{f}_{m,i}(t,x) at left boundary
real (DP), dimension (:), allocatable  :: TilFmi1Right  ! values of \tilde{f}_{m,i}(t,x) at right boundary
                                ! at the right and left boundary points, TilFmiRight(m), TilFmiLeft(m)
                                ! -- m is the index in the basis functions/nodes  in u
                                
real (DP), dimension (:,:,:), allocatable :: r1!  temp varaibles to calculate the contribution from the right side
                                 ! (p,m,j)
                                 ! -- p is the index in the basis functions in x 
                                 ! -- m is the index in the basis functions in u
                                 ! -- j is the cell in x
integer :: loc_alloc_stat ! to keep allocation status                                 

!!!!!!!!!!!!!!
! k -- order of the polynomial interpolation in x 
! N  --- umber of cells in x 
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! we use constants from common_variables_mod to select between different available types of boundary 
! conditions and different boundary data (if available)
!!!!!!!!!!!
!! Evaluation of the spatial operator:
!! ADD VOLUME TERMS:
f1 = VolTermsOrds_HOV1D3D(ftil1,tau,k,size(ftil1,2),N,nodes_u_loc)
!! ADD FLUX TERMS INCLUDING BCs

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!! Modified Alex 05/15/09  !!!!!!!!!!!!!!!!!
allocate (TilFmi1Left(1:size(ftil1,2)),TilFmi1Right(1:size(ftil1,2)), stat=loc_alloc_stat)
if (loc_alloc_stat >0) then 
 print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI: Allocation error for variable  (TilFmi1Left,TilFmi1Right)"
 stop
end if
! First, we need to compute the values of the function at the boudaries
if (selected_leftbcond == selected_rightbcond) then 
! if the boundary conditions are the same type on both boundaries 
select case (selected_leftbcond)
 case(periodic_bc) 
     call PeriodicBCsLeg(TilFmi1Left,TilFmi1Right,xmesh,ftil1)
 case(exact_bc)  
   select case (selected_exact_sol)
       case(1)
     call ExactBCs1D3DLegOrds (TilFmi1Left,TilFmi1Right,xmesh,curr_time,f_1D3D_u_vectors,& 
                                        nodes_u_loc,nodes_v_loc,nodes_w_loc) ! the same as case 2
       case(2)
     call ExactBCs1D3DLegOrds (TilFmi1Left,TilFmi1Right,xmesh,curr_time,f_1D3D_u_vectors,&
                                        nodes_u_loc,nodes_v_loc,nodes_w_loc) ! the same as case 1
       case default 
	 print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI:Unsupported type of exact solution"
	 TilFmi1Left = 0
	 TilFmi1Right = 0
   end select
case (diffusive_reflection_bc) 
    print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI: Diffusion Boundary conditions has not been implemented yet. Stop"
    stop
    !call DiffuiveBCsLeftHOV1D(TilFmi1Left,ftil1(:,:,1)) 
    ! call DiffuiveBCsRightHOV1D(TilFmi1Right,ftil1(:,:,N)) 
case default 
		    print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI: Unsupported Type of Boundary Conditions"
		    stop
end select 
else 
print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI:Different BCs on opposite boundaries have not been implemented yet!"
		    stop
end if     
! Now when the solution at the boundary is computed, we can use it in the numerical flux... 
f1 = f1 - FluxTermsUpwindOrds_HOV1D3D (ftil1,TilFmi1Right,TilFmi1Left,tau,k,size(nodes_u_loc,1),N,nodes_u_loc) 
deallocate (TilFmi1Right,TilFmi1Left)
!! ADD THE RIGHT SIDE (COLLISION TERMS)
!! COMMENTED with !* IF FREE MOLECULAR FLOW IS DESIRED (Alex, 05/19/2010) 
! allocate space for the right side values
allocate (r1(0:size(ftil1,1)-1, size(ftil1,2), size(ftil1,3)), stat=loc_alloc_stat)
   if (loc_alloc_stat >0) then 
   print *, "SpatialOperatorHOV1D3D_UpwindFlux_MPI: SpatialOperatorHOV1D3D_UpwindFlux: Allocation error for variable (r1)"
   end if 
! evaluate the right side   
call RightSide1D3D_HOV1D_MPI (r1,ftil1,tau,curr_time)
f1 = f1 + r1
! deallocate temporary variables r1
deallocate (r1)
!!! End Spatial Operator
end subroutine SpatialOperatorHOV1D3D_UpwindFlux_MPI


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! RightSide_HOV1D_MPI (ftil1,tau)
! 
! This function calculates the right side of the 1D BKG model: (see notes)
!
! This function is a modification of RightSide_HOV1D to adapt for MPI parallelization
!
!
!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
subroutine RightSide1D3D_HOV1D_MPI (r1,ftil1,tau,time)

use common_variables_mod, only: moments_x_gauss_order, moments_refine_x,&
                                XWeightLegPol,XLegPol
                      
use spectral_tools_mod

!!!!!!!!! link to the DGVlib

use DGV_collision_mod
use DGV_mpiroutines


!!!!!!!!!


!!! main variables
real (DP), dimension (0:,:,:), intent (out) :: r1 ! values of the right side multiplied by \tau, 
                                   ! have the same dimensions as the main variable
                                   ! main variable -- coefficients of spectral decomposition in 
                                   ! characteristic variables  ftil(p,m_count,j):
                                   ! -- p is the index in the basis functions in x 
                                   ! -- m_count is the index in the basis functions in u
                                   ! -- j is the cell in x
real (DP), dimension (0:,:,:), intent (in) :: ftil1 ! main variable -- coefficients of spectral decomposition in 
                                   ! characteristic variables  ftil(p,m_count,j):
                                   ! -- p is the index in the basis functions in x 
                                   ! -- m_count is the index in the basis functions in u
                                   ! -- j is the cell in x

real (DP), intent (in) :: tau      ! time step parameter. Is used in time integration. 
real (DP), intent (in) :: time     ! tie value of the current time variable
!!! auxiliary variables: 
integer (I4B) :: i,j,p,q ! some local counters
! real (DP) :: dx ! some local variables 
integer :: loc_alloc_stat ! to keep allocation status		

real (DP), dimension (:,:), allocatable :: RS1c ! RS1c(l,p) - temp variable to store values of the right side of the kinetic equation on one cell
                                ! p --- number of the node on the cell (uncluding the refinement in x)
						        ! l -- node in velocity

real (DP), dimension (:), allocatable :: fcol,f ! temp variable to store \value of the collision operator at a single point x 
						        
integer (I4B) :: num_x_node ! scrap variable to keep the number of the nodal point x where solution is evaluated to compute galerkin coefficients()

!!!!!!!!!!!!!!!!!
r1=0 ! reset the right side
!!!
! creating the scrap variables -- we assume that all cells in x will have the same amount of Galerkin coefficients.
! and that the same amount of Gauss nodes is used for computing Galerkin coefficients on each cell. 
allocate (RS1c(moments_x_gauss_order*moments_refine_x,size(ftil1,2)), stat=loc_alloc_stat)
 !
 if (loc_alloc_stat >0) then 
  print *, "RightSide1D3D_HOV1D: Allocation error for variable (RS1c)"
 end if 
 !
allocate (f(size(ftil1,2)),fcol(size(ftil1,2)), stat=loc_alloc_stat)
 !
 if (loc_alloc_stat >0) then 
  print *, "RightSide1D3D_HOV1D: Allocation error for variable (f), (fcol)"
 end if 
 !
do j = 1,size(ftil1,3) ! we will have a loop in all cells in x
 ! first,we assemble the solution at all Gauss nodes in x at all velocity nodes in u
 RS1c = 0 ! reset the value of the solution on one cell:
 do i = 1,size(ftil1,2) ! loop in all velocity nodes
  do q = 1,moments_x_gauss_order*moments_refine_x ! loop in points x on the cell 
   do p = 0,size(ftil1,1)-1 ! loop in galerkin coefficients
    !! Evaluate the basis functions \varhi(x) on xx 
    RS1c(q,i) = RS1c(q,i) + XLegPol(q,p)*ftil1(p,i,j)
   end do !end loop in Galerkin coefficients
  end do ! end loop in points x on the cell
 end do ! end loop in all velocity nodes 
 ! the solution is assembled at all points x on one cell in x from the Galerkin coefficients! 
 ! now, we can pass the solutoin to the subroutine evaluating the right hand side.
 do q = 1,moments_x_gauss_order*moments_refine_x ! loop in points x on the cell ! size(Xnodes_unit,1) = moments_x_gauss_order*moments_refine_x
  num_x_node = (j-1)*moments_x_gauss_order*moments_refine_x + q 
  do i = 1,size(ftil1,2) ! loop in all velocity nodes
   f(i) = RS1c(q,i);  ! place solution at one point in x in a temp variable
  end do ! end loop in all velocity nodes
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  !! CALL DGVlib subroutine 
  call UniversalCollisionOperator1DonecellDGV_MPI(f,fcol,time,tau,num_x_node) ! fcol returns the collision operator multiplied by tau
        ! also, a lot of constants, including the gas constants and the dimensionless reduction constants are used from DGVcommvar 
        ! the constants need to be set up to work properly... 
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  do i = 1,size(ftil1,2) ! loop in all velocity nodes
   RS1c(q,i) = fcol(i) ! the value of the collision integral is saved in the temp array 
  end do ! end loop in all velocity nodes
 end do ! end loop in points x on the cell 
 ! lastly, we compute the Galerkin coefficients of the right hand side:
 do i = 1,size(ftil1,2) ! loop in all velocity nodes
  do p = 0,size(ftil1,1)-1 ! loop in galerkin coefficients
   r1(p,i,j) = sum(XWeightLegPol(:,p)*RS1c(:,i))*(2*p+1)/Real(2*moments_refine_x,DP)  ! the right side must have the collision frequency and the time step in it already 
  end do  ! end loop in Galerkin coefficients
 end do ! end loop in all velocity nodes
end do ! end of the loop in all cells in x
!! Kinda done ... 
deallocate (RS1c,f,fcol)

end subroutine RightSide1D3D_HOV1D_MPI




end module mpi_routines
